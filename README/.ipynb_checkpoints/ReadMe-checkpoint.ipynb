{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#How does ReadNotes work#\n",
    "\n",
    "##Structure of the project##\n",
    "\n",
    "All python scripts are saved in the folder SCRIPTS. The original note files are in the folder ORIGINALS and \n",
    "should never be changed. The preprocessed files are saved in the folder PREPROCESSED. In PREPROCESSED\n",
    "there are four different folders for the different steps of the preprocess.\n",
    "\n",
    "##Preprocess: From scanned notes to single note files##\n",
    "\n",
    "###Rotate image###\n",
    "The first step of the preprocess is handled by the python file auto_rotate_scanned_notes.py\n",
    "This takes the note files from ORIGINALS and rotates the whole file such that the staves are horizontal.\n",
    "The rotated files are saved in PREPROCESSED/STRAIGHT. \n",
    "\n",
    "Original image\n",
    "![Original image](figures/im1.jpg)\n",
    "\n",
    "Straight image\n",
    "![Straight image](figures/im2.jpg)\n",
    "\n",
    "###Shrink image size###\n",
    "Mostly the size of a scanned image is large. This would make the process and calculation very longly.\n",
    "It is sufficient precise, if we shrink the images to a smaller size.\n",
    "We simply use the programm 'convert' to shrink the image size to 1000 pixel horizontally - of course the number of pixel on the vertical axis depends because 'convert' keeps the ratio. The shrinked images are saved in the folder PREPROCESS/SHRINKED\n",
    "\n",
    "###Find staves###\n",
    "Next we want to separate the single staves from each other. With python we open the note file and convert the input to only black-white. Usually colors don't carry information regarding notes. We read the jpge input as array and obtain a matrix with numbers between 0 (black) and 255 (white)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[231 238 249 255 255 255 255 253 255 255 255 253 249 244 241 240 227 227]\n",
      " [233 239 248 255 255 255 249 245 255 255 255 252 250 248 246 245 233 233]\n",
      " [235 240 247 251 250 243 235 229 246 246 247 248 249 250 251 251 240 240]\n",
      " [238 241 245 245 239 228 216 209 224 226 231 236 242 247 251 254 243 243]\n",
      " [242 243 243 238 227 212 196 187 193 197 205 215 226 236 244 248 239 239]\n",
      " [245 245 241 232 216 196 178 166 157 163 174 188 204 218 229 235 227 227]\n",
      " [248 246 240 227 208 185 164 151 124 131 145 163 182 200 213 221 213 213]\n",
      " [249 246 239 225 203 178 156 142 104 113 128 147 168 188 203 211 204 204]\n",
      " [250 247 241 233 225 217 212 208 151 168 192 209 212 198 177 163 208 208]\n",
      " [252 247 239 229 218 208 200 196 161 175 194 206 205 189 168 154 182 182]\n",
      " [253 248 237 224 209 195 185 179 179 188 199 203 197 179 158 144 217 217]\n",
      " [254 248 236 221 205 190 179 172 200 204 207 204 194 176 158 147 227 227]\n",
      " [252 247 237 224 211 198 189 183 221 221 218 211 200 187 175 168 182 182]\n",
      " [248 245 240 233 226 219 214 211 239 235 230 223 216 210 206 204 207 207]\n",
      " [244 244 244 243 243 243 243 243 250 246 240 236 234 236 240 243 255 255]\n",
      " [241 243 246 250 255 255 255 255 255 252 247 244 246 253 255 255 251 251]\n",
      " [250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250]]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "song = Image.open('figures/im3.jpg').convert('L')\n",
    "songM = np.array(song)\n",
    "print songM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We exploit the fact that staves basically are composed of five black lines, which cross the whole horizontal. The darker the color, the smaller the numbers of the matrix. \n",
    "This means, by adding up the columns of the matrix, we obtain a vector that has five minima per stave; one for each line of the stave.\n",
    "In the following figure we see the normalized vector in blue and see for each of the three stave the five lines.\n",
    "\n",
    "![](figures/im4.jpg)\n",
    "\n",
    "We smooth the blue curve using Laplacian smoothing and obtain the green line.\n",
    "This is done such that we mathematically 'summarize' the five black lines to one whole. This green line has three maxima and we cut the line exactly at the maxima. \n",
    "The indices of the cuts are also used to cut the jpeg matrix.\n",
    "Additionally, the separate matrices are verified to be staves and not the song header, text or anything else. Redundant white space from left, right, top and bottom is removed and finally we save the separated matrix pieces as jpg and obtain the single staves in the folder PREPROCESS/STAVES.\n",
    "\n",
    "Here as an example the first stave:\n",
    "![](figures/im5.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Find notes###\n",
    "The last step in the preprocess is to separate the single objects appearing in a stave from each other and save the object in single jpg files. This can be notes, clefs, rests, bars and so on. \n",
    "We open the jpg-file with the staves as matrix and again exploit the fact that the objects in the lines are black and hence are presented by smaller numbers in the matrix. This time we sum up the rows of the matrix and obtain a horizontal vector. After normalizing the vector, the five black lines of the staves do not impair the recognition of the objects because the lines go constantly through the whole image and cancel out. \n",
    "We regard the vector as a function and smooth it using Laplacian smoothing. This is done to avoid little deflections affecting the process. Now we search for maxima. In the following figure, the blue curve is the rows summed up and the green curve is the smoothed vector.\n",
    "\n",
    "![](figures/im8.jpg)\n",
    "\n",
    "Cutting the matrix at the maxima found using the green line, we obtain the objects appearing in the stave. \n",
    "We want that all jpg file containing the objects have the same size and add or remove therefore some white space.\n",
    "Finally the objects are saved as jpg files in PREPROCESS/NOTES. Some samples are plotted below.\n",
    "\n",
    "![1](figures/im9.jpg)  | ![2](figures/im10.jpg) | ![3](figures/im11.jpg) | ![4](figures/im12.jpg) | ![5](figures/im13.jpg)\n",
    ":-------------:|:--------------:|:--------------:|:--------------:|:--------------:\n",
    "![6](figures/im14.jpg) | ![7](figures/im15.jpg) | ![8](figures/im16.jpg) | ![9](figures/im17.jpg) | ![10](figures/im18.jpg)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify: From single note files to note-or-not-note recognition\n",
    "\n",
    "###Make training set###\n",
    "The aim of the following steps is to build a classifier using Google's library Tensorflow which regonizes if a note file contains a note or a not-note. The not-note could be a clef, rest, bar,..\n",
    "Therefore we first need a large enough training set. The note file, we just created, must be split into notes and not-notes. We need to do this by hand because the computer cannot do this yet (That's just the point) and make a folder YES with all the notes and a folder NO with all the not-notes.\n",
    "\n",
    "To multiply the number of training examples the python function create_variants creates for each file several other files with slightly changes content. The slight changes are shifting the image a few pixels up, down, left or right, bluring and clearing the image. \n",
    "Now we have a training set of over 28000 samples.\n",
    "\n",
    "###From jpg files to mathematical representation###\n",
    "To represent the jpg files in a mathematical way, the python function notes_to_csv.py writes a csv file X_notes.csv and inserts for each note file a row.\n",
    "The row contains the jpg as matrix of size 32 x 80 reshaped to a vector of size 32*80. The vector is normalized and only contains numbers between 0 and 1.\n",
    "In another csv file Y_notes.csv the row entry is simply 1 or 0 depending if the note file contains a note or a not-note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Building the classifier###\n",
    "\n",
    "####Logistic regression####\n",
    "First we try a simple neuronal network with just two layers: input and output layer. The size of the input layer is 2560, which is the amount of pixel per note file. The size of the output layer is 2, which stands for note and not-note. That means, we have only one weight matrix W of the size 2x2560 and a bias vector b of size 2. \n",
    "This procedure is equivalent to logistic regression.\n",
    "\n",
    "To use Tensorflow, the training samples must be vectors with entries between 0 and 1, the training labels also must be vectors with one 1 and else 0 emphasizing the i-th entry. \n",
    "\n",
    "We define placeholders x and y_ for the training samples and the training labels, the variables W (weight matrix) and b (bias vector), the cost function using logistic regression and the optimization-algorithm and -accuracy.\n",
    "\n",
    "In Tensorflow, the training steps obtain the training samples in separated batches to not overuse the capacity of the computer. \n",
    "\n",
    "After the classifier is build, we check its quality with the test set and see that we obtain an accuracy of..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####Multilayer neural network####\n",
    "The accuracy of the logistic regression is not very good. Without hidden layers the network is not able to recognize shapes or neglect the size of the note. Therefore we build another neural network with several layers and weight matrices.\n",
    "\n",
    "We exploit the 2D shape of the note file and reshape the input layer to a matrix of size 80x32. After the first convolution which transforms the input layer to a 32x80x32 matrix as second layer, ReLu *f(x) = max(0,x)* is used for rectification and max_pool_2x2 is used to shrink the size of the second layer to  40x16x32. \n",
    "\n",
    "The second convolution transforms the second layer to a 64x40x16 matrix as third layer. Again ReLu is used for rectification and max_pool_2x2 shrinks the third layer to size 20x8x64.\n",
    "\n",
    "We continue by reshaping the third layer back to a vector of size 20\\*8\\*64 and use a simple weight matrix of size 1024x20\\*8\\*64 and a bias vector of size 1024 to obtain the forth layer of size 1024 by matrix multiplication. Again we use ReLu.\n",
    "In this layer we also use a dropout to randomly cancel out features to avoid overfitting.\n",
    "\n",
    "Finally, we obtain the output layer by matrix multiplication with a weight matrix of size 2x1024 and bias vector of size 2. This time we use the sigmoid function softmax instead of ReLu as the last step.\n",
    "\n",
    "As costfunction we take logistic regression and as optimization accuracy 1e-4.\n",
    "\n",
    "After the classifier is build, we check its quality with the test set and see that the accuracy improved to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
